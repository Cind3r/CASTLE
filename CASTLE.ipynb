{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888617a0",
   "metadata": {},
   "source": [
    "# C.A.S.T.L.E.\n",
    "\n",
    "#### Component Assembly Structure Tracking for Learning Emergence\n",
    "\n",
    "Castle is an assembly tracking neural network, which partitions weight tensors into `molecular blocks` and assigns `atomic codes` based on block statistics. The resulting `molecular lattice` encodes the network's structural motifs, enabling calculation of an assembly index that quantifies **modularity and reuse**. During training, the `loss function` incorporates a `complexity reward` or penalty derived from the assembly index, and gradients are modulated according to molecular reuse. If the assembly index exceeds a threshold, the architecture is evolved to favor more efficient or interpretable structures. This approach bridges neural network optimization with principles from assembly theory, promoting modularity, interpretability, and adaptive architectural evolution.\n",
    "\n",
    "***\n",
    "\n",
    "## Mathematical Model of Assembly Tracking Neural Network with Atomic Codes\n",
    "\n",
    "## 1. Weight Tensor Partitioning\n",
    "\n",
    "Let $W \\in \\mathbb{R}^{m \\times n}$ be the weight matrix of a neural network layer, where:\n",
    "- $m$: number of input features,\n",
    "- $n$: number of output features.\n",
    "\n",
    "Partition $W$ into non-overlapping blocks (molecules) of size $k \\times k$:\n",
    "\n",
    "$$W = \\bigcup_{i=1}^{M} \\bigcup_{j=1}^{N} W_{i,j}$$\n",
    "\n",
    "where:\n",
    "- $W_{i,j}$: block (molecule) at position $(i, j)$,\n",
    "- $M = \\lfloor m/k \\rfloor$: number of blocks along rows,\n",
    "- $N = \\lfloor n/k \\rfloor$: number of blocks along columns,\n",
    "- $k$: molecule (block) size.\n",
    "\n",
    "## 2. Atomic Code Assignment\n",
    "\n",
    "For each molecule $W_{i,j}$, compute its average value:\n",
    "\n",
    "$$\\mu_{i,j} = \\frac{1}{k^2} \\sum_{a=1}^{k} \\sum_{b=1}^{k} W_{i,j}[a, b]$$\n",
    "\n",
    "where:\n",
    "- $\\mu_{i,j}$: average value of molecule $W_{i,j}$,\n",
    "- $a, b$: indices within the block.\n",
    "\n",
    "Assign an atomic code $S_{i,j}$ using a discretization function $f$:\n",
    "\n",
    "$$S_{i,j} = f(\\mu_{i,j})$$\n",
    "\n",
    "where:\n",
    "- $S_{i,j}$: atomic code assigned to molecule $W_{i,j}$,\n",
    "- $f$: function mapping average value to a symbol (e.g., 'Ze', 'Sm', 'Md').\n",
    "\n",
    "## 3. Lattice Construction\n",
    "\n",
    "The set of atomic codes forms a molecular lattice $L$:\n",
    "\n",
    "$$L = \\{ S_{i,j} \\mid 1 \\leq i \\leq M, 1 \\leq j \\leq N \\}$$\n",
    "\n",
    "where:\n",
    "- $L$: molecular lattice,\n",
    "- $S_{i,j}$: atomic code at position $(i, j)$.\n",
    "\n",
    "## 4. Assembly Index Calculation\n",
    "\n",
    "Define the assembly index $A(L)$ as the minimal number of unique atomic codes or assembly steps required to construct $L$:\n",
    "\n",
    "$$A(L) = \\min \\left( \\text{number of steps to assemble } L \\text{ from atomic codes and reused sub-lattices} \\right)$$\n",
    "\n",
    "where:\n",
    "- $A(L)$: assembly index of lattice $L$,\n",
    "- \"steps\": count of unique codes and reused patterns needed for construction.\n",
    "\n",
    "## 5. Complexity Reward in Loss Function\n",
    "\n",
    "During training, modify the loss function to reward or penalize assembly complexity:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{base}} - \\lambda \\cdot R(A(L))$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{L}_{\\text{total}}$: total loss,\n",
    "- $\\mathcal{L}_{\\text{base}}$: standard loss (e.g., binary cross-entropy),\n",
    "- $R(A(L))$: reward function based on assembly index,\n",
    "- $\\lambda$: hyperparameter controlling reward strength.\n",
    "\n",
    "## 6. Gradient Modification\n",
    "\n",
    "Scale gradients for each molecule according to reuse and complexity:\n",
    "\n",
    "$$\\nabla W_{i,j} \\leftarrow \\nabla W_{i,j} \\cdot g(S_{i,j}, A(L))$$\n",
    "\n",
    "where:\n",
    "- $\\nabla W_{i,j}$: gradient for molecule $W_{i,j}$,\n",
    "- $g$: modifier function (e.g., reduces updates for highly reused molecules),\n",
    "- $S_{i,j}$: atomic code,\n",
    "- $A(L)$: assembly index.\n",
    "\n",
    "## 7. Architecture Evolution\n",
    "\n",
    "If assembly index exceeds a threshold, suggest architectural changes:\n",
    "\n",
    "$$\\text{If } A(L) > T, \\text{ then modify layer sizes or add/remove layers}$$\n",
    "\n",
    "where:\n",
    "- $T$: complexity threshold for architectural evolution,\n",
    "- $A(L)$: assembly index.\n",
    "\n",
    "## 8. System Complexity\n",
    "\n",
    "### 8.1 Assembly Theory Metric\n",
    "\n",
    "$$A_{\\text{sys}}^{(t)} = \\frac{1}{|\\mathcal{P}|} \\sum_{i=1}^{|\\mathcal{P}|} e^{a_i} \\cdot \\frac{n_i - 1}{|\\mathcal{P}|}$$\n",
    "\n",
    "Where:  \n",
    "- $A_{\\text{sys}}^{(t)}$: System assembly complexity at time $t$  \n",
    "- $|\\mathcal{P}|$: Population size  \n",
    "- $a_i$: Assembly complexity of module $i$  \n",
    "- $n_i$: Copy number of module type $i$  \n",
    "\n",
    "## 9. Refined Assembly Theory Metric for Neural Networks\n",
    "\n",
    "To better capture the modularity and reuse in neural networks, we refine the assembly theory metric by explicitly accounting for the diversity and recurrence of molecular patterns (atomic codes) in the lattice:\n",
    "\n",
    "Let $\\mathcal{S} = \\{ S_{i,j} \\}$ be the set of atomic codes in lattice $L$, and let $u_s$ be the number of unique codes, $n_s$ the copy number of code $s$, and $a_s$ the assembly complexity of code $s$.\n",
    "\n",
    "Define the refined system assembly complexity as:\n",
    "\n",
    "$$A_{\\text{sys}}^{(t)} = \\frac{1}{u_s} \\sum_{s \\in \\mathcal{S}} e^{a_s} \\cdot \\frac{n_s - 1}{|\\mathcal{S}|}$$\n",
    "\n",
    "where:\n",
    "- $u_s$: number of unique atomic codes in $L$,\n",
    "- $n_s$: number of occurrences of code $s$ in $L$,\n",
    "- $a_s$: assembly complexity of code $s$ (e.g., minimal steps to construct $s$ from primitives),\n",
    "- $|\\mathcal{S}|$: total number of molecules in $L$.\n",
    "\n",
    "**Interpretation:**  \n",
    "This metric rewards reuse (high $n_s$) and penalizes diversity (high $u_s$), while weighting by the intrinsic complexity $a_s$ of each code. The exponential term amplifies the impact of complex codes, and the normalization by $u_s$ and $|\\mathcal{S}|$ ensures comparability across networks.\n",
    "\n",
    "**Application in Training:**  \n",
    "- Use $A_{\\text{sys}}^{(t)}$ as a regularizer in the loss function to promote modularity and efficient reuse.\n",
    "- Track $A_{\\text{sys}}^{(t)}$ over epochs to monitor the evolution of network complexity and modularity.\n",
    "- Suggest architectural changes if $A_{\\text{sys}}^{(t)}$ exceeds a threshold, indicating excessive diversity or insufficient reuse.\n",
    "\n",
    "## 10. Complexity-Accuracy Relationship\n",
    "\n",
    "Let $\\mathcal{A}(t)$ denote the model accuracy at time $t$, and $A_{\\text{sys}}^{(t)}$ represent the system assembly complexity. The relationship between accuracy and complexity can be modeled as:\n",
    "\n",
    "$$\\mathcal{A}(t) = \\mathcal{A}_{\\text{base}} + \\gamma \\cdot \\left(1 - e^{-\\beta \\cdot A_{\\text{sys}}^{(t)}}\\right)$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{A}_{\\text{base}}$: base accuracy achievable with minimal complexity,\n",
    "- $\\gamma$: maximum potential accuracy improvement from complexity,\n",
    "- $\\beta$: rate parameter controlling how quickly complexity benefits manifest.\n",
    "\n",
    "This formulation captures three key properties:\n",
    "1. As $A_{\\text{sys}}^{(t)} \\to 0$, $\\mathcal{A}(t) \\to \\mathcal{A}_{\\text{base}}$,\n",
    "2. As $A_{\\text{sys}}^{(t)} \\to \\infty$, $\\mathcal{A}(t) \\to \\mathcal{A}_{\\text{base}} + \\gamma$,\n",
    "3. The marginal benefit of additional complexity diminishes as complexity increases, reflecting the principle of diminishing returns.\n",
    "\n",
    "### Alternative Formulation for Small Complexity Ranges\n",
    "\n",
    "For practical application with small ranges of complexity, a linear approximation may be suitable:\n",
    "\n",
    "$$\\mathcal{A}(t) \\approx \\mathcal{A}_{\\text{min}} + \\delta \\cdot A_{\\text{sys}}^{(t)}$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{A}_{\\text{min}}$: minimum accuracy with zero complexity,\n",
    "- $\\delta$: linear rate of accuracy improvement with complexity.\n",
    "\n",
    "## 11. Empirical Validation\n",
    "\n",
    "This relationship can be validated by:\n",
    "1. Measuring $A_{\\text{sys}}^{(t)}$ and $\\mathcal{A}(t)$ at multiple training epochs,\n",
    "2. Fitting the model parameters ($\\mathcal{A}_{\\text{base}}$, $\\gamma$, $\\beta$) or ($\\mathcal{A}_{\\text{min}}$, $\\delta$),\n",
    "3. Testing predictive power on held-out model configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fe9d99",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d540fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.datasets import load_breast_cancer, load_iris, load_wine\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# self-made imports\n",
    "from core.trainer import train_complexity_aware, train_basic, train_reward_complexity\n",
    "from core.analyzer import validate_complexity_accuracy_relationship, analyze_complexity_accuracy_relationship\n",
    "from visualize.lattice_builder import plot_pyvis_3d_lattice_interactive\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab850c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e7742_row0_col0, #T_e7742_row0_col1, #T_e7742_row0_col2, #T_e7742_row0_col3, #T_e7742_row1_col0, #T_e7742_row1_col1, #T_e7742_row1_col2, #T_e7742_row1_col3, #T_e7742_row2_col0, #T_e7742_row2_col1, #T_e7742_row2_col2, #T_e7742_row2_col3, #T_e7742_row3_col0, #T_e7742_row3_col1, #T_e7742_row3_col2, #T_e7742_row3_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e7742\" style=\"width: 100%; border-collapse: collapse;\">\n",
       "  <caption>Dataset Information</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Dataset</th>\n",
       "      <th id=\"T_e7742_level0_col0\" class=\"col_heading level0 col0\" >Breast Cancer</th>\n",
       "      <th id=\"T_e7742_level0_col1\" class=\"col_heading level0 col1\" >Iris</th>\n",
       "      <th id=\"T_e7742_level0_col2\" class=\"col_heading level0 col2\" >Wine</th>\n",
       "      <th id=\"T_e7742_level0_col3\" class=\"col_heading level0 col3\" >MNIST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e7742_level0_row0\" class=\"row_heading level0 row0\" >Shape of Training Data</th>\n",
       "      <td id=\"T_e7742_row0_col0\" class=\"data row0 col0\" >(455, 30)</td>\n",
       "      <td id=\"T_e7742_row0_col1\" class=\"data row0 col1\" >(120, 4)</td>\n",
       "      <td id=\"T_e7742_row0_col2\" class=\"data row0 col2\" >(142, 13)</td>\n",
       "      <td id=\"T_e7742_row0_col3\" class=\"data row0 col3\" >(60000, 784)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e7742_level0_row1\" class=\"row_heading level0 row1\" >Number of Classes</th>\n",
       "      <td id=\"T_e7742_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_e7742_row1_col1\" class=\"data row1 col1\" >3</td>\n",
       "      <td id=\"T_e7742_row1_col2\" class=\"data row1 col2\" >3</td>\n",
       "      <td id=\"T_e7742_row1_col3\" class=\"data row1 col3\" >2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e7742_level0_row2\" class=\"row_heading level0 row2\" >Number of Features</th>\n",
       "      <td id=\"T_e7742_row2_col0\" class=\"data row2 col0\" >30</td>\n",
       "      <td id=\"T_e7742_row2_col1\" class=\"data row2 col1\" >4</td>\n",
       "      <td id=\"T_e7742_row2_col2\" class=\"data row2 col2\" >13</td>\n",
       "      <td id=\"T_e7742_row2_col3\" class=\"data row2 col3\" >784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e7742_level0_row3\" class=\"row_heading level0 row3\" >Classes</th>\n",
       "      <td id=\"T_e7742_row3_col0\" class=\"data row3 col0\" >[0 1]</td>\n",
       "      <td id=\"T_e7742_row3_col1\" class=\"data row3 col1\" >[0 1 2]</td>\n",
       "      <td id=\"T_e7742_row3_col2\" class=\"data row3 col2\" >[0 1 2]</td>\n",
       "      <td id=\"T_e7742_row3_col3\" class=\"data row3 col3\" >[0. 1.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1f0812b02c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Parameters\n",
    "test_size = 0.2         # Proportion of the dataset to include in the test split (default for all functions is 0.2))\n",
    "random_state = 42\n",
    "\n",
    "# Data Loading Functions\n",
    "def load_breast_cancer_data(test_size=0.2, random_state=42):\n",
    "    # Load the breast cancer dataset\n",
    "    data = load_breast_cancer()\n",
    "    X, y = data.data, data.target\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def load_iris_data(test_size=0.2, random_state=42):\n",
    "    # Load the iris dataset\n",
    "    data = load_iris()\n",
    "    X, y = data.data, data.target\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def load_wine_data(test_size=0.2, random_state=42):\n",
    "    # Load the wine dataset\n",
    "    data = load_wine()\n",
    "    X, y = data.data, data.target\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def load_mnsit_data():\n",
    "    # Load the MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "    x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "# Output\n",
    "try:\n",
    "    X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = load_breast_cancer_data(test_size, random_state)\n",
    "    X_train_iris, X_test_iris, y_train_iris, y_test_iris = load_iris_data(test_size, random_state)\n",
    "    X_train_wine, X_test_wine, y_train_wine, y_test_wine = load_wine_data(test_size, random_state)\n",
    "    X_train_mnist, X_test_mnist, y_train_mnist, y_test_mnist = load_mnsit_data()\n",
    "    print(\"Data loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the data: {e}\")\n",
    "\n",
    "# Print pd dataframe of dataset's shape, classes, and number of features\n",
    "pd.DataFrame({\n",
    "    \"Dataset\": [\"Breast Cancer\", \"Iris\", \"Wine\", \"MNIST\"],\n",
    "    \"Shape of Training Data\": [X_train_cancer.shape, X_train_iris.shape, X_train_wine.shape, X_train_mnist.shape],\n",
    "    \"Number of Classes\": [len(np.unique(y_train_cancer)), len(np.unique(y_train_iris)), len(np.unique(y_train_wine)), len(np.unique(y_train_mnist))],\n",
    "    \"Number of Features\": [X_train_cancer.shape[1], X_train_iris.shape[1], X_train_wine.shape[1], X_train_mnist.shape[1]],\n",
    "    \"Classes\": [np.unique(y_train_cancer), np.unique(y_train_iris), np.unique(y_train_wine), np.unique(y_train_mnist)]\n",
    "}).set_index(\"Dataset\").T.applymap(lambda x: x if isinstance(x, str) else str(x)).style.set_properties(**{'text-align': 'left'}).set_table_attributes('style=\"width: 100%; border-collapse: collapse;\"').set_caption(\"Dataset Information\").format(na_rep='N/A')   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582dea02",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Testing and Validation\n",
    "\n",
    "Let $\\mathcal{A}(t)$ denote the model accuracy at time $t$, and $A_{\\text{sys}}^{(t)}$ represent the system assembly complexity. The relationship between accuracy and complexity can be modeled as:\n",
    "\n",
    "$$\\mathcal{A}(t) = \\mathcal{A}_{\\text{base}} + \\gamma \\cdot \\left(1 - e^{-\\beta \\cdot A_{\\text{sys}}^{(t)}}\\right)$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{A}_{\\text{base}}$: base accuracy achievable with minimal complexity,\n",
    "- $\\gamma$: maximum potential accuracy improvement from complexity,\n",
    "- $\\beta$: rate parameter controlling how quickly complexity benefits manifest.\n",
    "\n",
    "This formulation captures three key properties:\n",
    "1. As $A_{\\text{sys}}^{(t)} \\to 0$, $\\mathcal{A}(t) \\to \\mathcal{A}_{\\text{base}}$,\n",
    "2. As $A_{\\text{sys}}^{(t)} \\to \\infty$, $\\mathcal{A}(t) \\to \\mathcal{A}_{\\text{base}} + \\gamma$,\n",
    "3. The marginal benefit of additional complexity diminishes as complexity increases, reflecting the principle of diminishing returns.\n",
    "\n",
    "#### Alternative Formulation for Small Complexity Ranges\n",
    "\n",
    "For practical application with small ranges of complexity, a linear approximation may be suitable:\n",
    "\n",
    "$$\\mathcal{A}(t) \\approx \\mathcal{A}_{\\text{min}} + \\delta \\cdot A_{\\text{sys}}^{(t)}$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{A}_{\\text{min}}$: minimum accuracy with zero complexity,\n",
    "- $\\delta$: linear rate of accuracy improvement with complexity.\n",
    "\n",
    ">```python\n",
    ">def exponential_model(complexity, A_base, gamma, beta):\n",
    ">    \"\"\"\n",
    ">    Exponential model for accuracy-complexity relationship:\n",
    ">    A(t) = A_base + γ * (1 - exp(-β * A_sys(t)))\n",
    ">    \"\"\"\n",
    ">    return A_base + gamma * (1 - np.exp(-beta * complexity))\n",
    ">\n",
    ">def linear_model(complexity, A_min, delta):\n",
    ">    \"\"\"\n",
    ">    Linear model for accuracy-complexity relationship:\n",
    ">    A(t) ≈ A_min + δ * A_sys(t)\n",
    ">    \"\"\"\n",
    ">    return A_min + delta * complexity\n",
    ">```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d080ac3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
